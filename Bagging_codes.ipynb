{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The codes used in the calculations \n",
    "#206 machine T18 all ReLU ran 100 times for all k folds (bagging code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "\n",
    "import time\n",
    "\n",
    "###################################\n",
    "\n",
    "def norm(x, train_dataset):\n",
    "    train_stats = train_dataset.describe().transpose()\n",
    "    return (x - train_stats['mean']) / train_stats['std'].replace(to_replace=0, value=1)\n",
    "\n",
    "def build_model(data):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(5, activation=tf.nn.relu, input_shape=(data.shape[1],)))\n",
    "    for k in range(3-1):\n",
    "        model.add(keras.layers.Dense(5, activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=keras.optimizers.Adam(),\n",
    "                metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "\n",
    "###################################\n",
    "\n",
    "CSN_path = './'\n",
    "\n",
    "def load_CSN_data():\n",
    "    csv_path = CSN_path + \"Master_List_LCPLCP.csv\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "CSN = load_CSN_data()\n",
    "\n",
    "CSN = CSN.drop(['Example ID', 'Source', 'Figure ID', 'Data Provider', 'PI',\n",
    "       'Date Received', 'Data Measurment Published', 'Prior Exposure', 'Comments', 'Error'], axis=1)\n",
    "\n",
    "tsize = CSN.shape[0]//10\n",
    "\n",
    "CSN_prepared = pd.get_dummies(CSN)\n",
    "\n",
    "CSN_prepared['Surface Area per Liter'] = CSN_prepared['Surface Area (NMC) (m2/g)'] * CSN_prepared['Concentration (mg/L)']\n",
    "CSN_prepared = CSN_prepared.drop(['Surface Area (NMC) (m2/g)'], axis=1)\n",
    "\n",
    "CSN_prepared['log Concentration'] = np.log10(CSN_prepared['Concentration (mg/L)'] + 1e-9)\n",
    "CSN_prepared = CSN_prepared.drop(['Concentration (mg/L)'], axis=1)\n",
    "\n",
    "CSN_prepared = CSN_prepared[:-18]\n",
    "\n",
    "###################################\n",
    "\n",
    "CSN_path = './'\n",
    "\n",
    "def load_CSN_data():\n",
    "    csv_path = CSN_path + \"Master_List_LCPLCP.csv\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "CSN = load_CSN_data()\n",
    "\n",
    "CSN_new_err = CSN['Error'][-18:]\n",
    "\n",
    "CSN = CSN.drop(['Example ID', 'Source', 'Figure ID', 'Data Provider', 'PI',\n",
    "       'Date Received', 'Data Measurment Published', 'Prior Exposure', 'Comments', 'Error'], axis=1)\n",
    "\n",
    "#tsize = CSN.shape[0]//10\n",
    "\n",
    "CSN_new = pd.get_dummies(CSN)\n",
    "\n",
    "CSN_new['Surface Area per Liter'] = CSN_new['Surface Area (NMC) (m2/g)'] * CSN_new['Concentration (mg/L)']\n",
    "CSN_new = CSN_new.drop(['Surface Area (NMC) (m2/g)'], axis=1)\n",
    "\n",
    "CSN_new['log Concentration'] = np.log10(CSN_new['Concentration (mg/L)'] + 1e-9)\n",
    "CSN_new = CSN_new.drop(['Concentration (mg/L)'], axis=1)\n",
    "\n",
    "CSN_new = CSN_new[-18:]\n",
    "\n",
    "###################################\n",
    "\n",
    "def ikfold(k, data, test, s):\n",
    "\n",
    "    CSN_shuf = data#sklearn.utils.shuffle(data, random_state=25)\n",
    "    valn = data.shape[0]//k\n",
    "    vscores = []\n",
    "    tscores = []\n",
    "    vloss = []\n",
    "    tloss = []\n",
    "    val_pred = np.zeros((k, test.shape[0]))\n",
    "    weights = np.zeros(k)\n",
    "\n",
    "    norm_train = data.drop(['Viability Fraction '], axis=1)\n",
    "\n",
    "    nCSN_test = norm(test.drop(['Viability Fraction '], axis=1),\n",
    "    norm_train)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(k):\n",
    "\n",
    "        if k == 1:\n",
    "            val = CSN_shuf\n",
    "            train = CSN_shuf\n",
    "        else:\n",
    "            val = CSN_shuf[valn*i:valn*(i+1)]\n",
    "            train = CSN_shuf[valn*(i+1):].append(CSN_shuf[:valn*i])\n",
    "\n",
    "        train_f = train.drop(['Viability Fraction '], axis=1)\n",
    "        val_f = val.drop(['Viability Fraction '], axis=1)\n",
    "\n",
    "        ntrain_f = norm(train_f, norm_train)\n",
    "        ntrain_l = train['Viability Fraction ']\n",
    "        nval_f = norm(val_f, norm_train)\n",
    "        nval_l = val['Viability Fraction ']\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        unique_seed = s[i]\n",
    "        np.random.seed(unique_seed)\n",
    "        tf.random.set_seed(unique_seed)\n",
    "\n",
    "        model = build_model(ntrain_f)\n",
    "\n",
    "        history = model.fit(ntrain_f,\n",
    "        ntrain_l,\n",
    "        validation_data=(nval_f, nval_l),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0)\n",
    "\n",
    "        #plot_mae(history)\n",
    "\n",
    "        val_pred[i] = model.predict(nCSN_test).flatten()\n",
    "        weights[i] = 1/history.history['val_mae'][-1]\n",
    "\n",
    "    WMean = np.average(val_pred, axis=0, weights=weights)\n",
    "    Werr = np.sqrt(np.average((WMean-val_pred)**2, weights=weights, axis=0))/np.sqrt(k)\n",
    "    print(k, time.time() - start_time)\n",
    "\n",
    "    return WMean, Werr\n",
    "\n",
    "###################################\n",
    "\n",
    "out = np.zeros((100, 4, 2, 18))\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    np.random.seed(i)\n",
    "    CSN_hold  = sklearn.utils.shuffle(CSN_prepared, random_state=np.random.randint(0, 100000))\n",
    "    out[i] = np.array([ikfold(i, CSN_hold, CSN_new, np.random.randint(0, 100000, i))\n",
    "                 for i in [1, 4, 7, 10]])\n",
    "\n",
    "    print((i+1), '% complete')\n",
    "    out.dump('206_T18.pkl')\n",
    "\n",
    "###################################\n",
    "\n",
    "out.dump('206_T18.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#206 machine T18 all ReLU ran 300 times for all k folds (bagging code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "\n",
    "import time\n",
    "\n",
    "###################################\n",
    "\n",
    "def norm(x, train_dataset):\n",
    "    train_stats = train_dataset.describe().transpose()\n",
    "    return (x - train_stats['mean']) / train_stats['std'].replace(to_replace=0, value=1)\n",
    "\n",
    "def build_model(data):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(5, activation=tf.nn.relu, input_shape=(data.shape[1],)))\n",
    "    for k in range(3-1):\n",
    "        model.add(keras.layers.Dense(5, activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=keras.optimizers.Adam(),\n",
    "                metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "\n",
    "###################################\n",
    "\n",
    "CSN_path = './'\n",
    "\n",
    "def load_CSN_data():\n",
    "    csv_path = CSN_path + \"Master_List_LCPLCP.csv\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "CSN = load_CSN_data()\n",
    "\n",
    "CSN = CSN.drop(['Example ID', 'Source', 'Figure ID', 'Data Provider', 'PI',\n",
    "       'Date Received', 'Data Measurment Published', 'Prior Exposure', 'Comments', 'Error'], axis=1)\n",
    "\n",
    "tsize = CSN.shape[0]//10\n",
    "\n",
    "CSN_prepared = pd.get_dummies(CSN)\n",
    "\n",
    "CSN_prepared['Surface Area per Liter'] = CSN_prepared['Surface Area (NMC) (m2/g)'] * CSN_prepared['Concentration (mg/L)']\n",
    "CSN_prepared = CSN_prepared.drop(['Surface Area (NMC) (m2/g)'], axis=1)\n",
    "\n",
    "CSN_prepared['log Concentration'] = np.log10(CSN_prepared['Concentration (mg/L)'] + 1e-9)\n",
    "CSN_prepared = CSN_prepared.drop(['Concentration (mg/L)'], axis=1)\n",
    "\n",
    "CSN_prepared = CSN_prepared[:-18]\n",
    "\n",
    "###################################\n",
    "\n",
    "CSN_path = './'\n",
    "\n",
    "def load_CSN_data():\n",
    "    csv_path = CSN_path + \"Master_List_LCPLCP.csv\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "CSN = load_CSN_data()\n",
    "\n",
    "CSN_new_err = CSN['Error'][-18:]\n",
    "\n",
    "CSN = CSN.drop(['Example ID', 'Source', 'Figure ID', 'Data Provider', 'PI',\n",
    "       'Date Received', 'Data Measurment Published', 'Prior Exposure', 'Comments', 'Error'], axis=1)\n",
    "\n",
    "#tsize = CSN.shape[0]//10\n",
    "\n",
    "CSN_new = pd.get_dummies(CSN)\n",
    "\n",
    "CSN_new['Surface Area per Liter'] = CSN_new['Surface Area (NMC) (m2/g)'] * CSN_new['Concentration (mg/L)']\n",
    "CSN_new = CSN_new.drop(['Surface Area (NMC) (m2/g)'], axis=1)\n",
    "\n",
    "CSN_new['log Concentration'] = np.log10(CSN_new['Concentration (mg/L)'] + 1e-9)\n",
    "CSN_new = CSN_new.drop(['Concentration (mg/L)'], axis=1)\n",
    "\n",
    "CSN_new = CSN_new[-18:]\n",
    "\n",
    "###################################\n",
    "\n",
    "def ikfold(k, data, test, s):\n",
    "\n",
    "    CSN_shuf = data#sklearn.utils.shuffle(data, random_state=25)\n",
    "    valn = data.shape[0]//k\n",
    "    vscores = []\n",
    "    tscores = []\n",
    "    vloss = []\n",
    "    tloss = []\n",
    "    val_pred = np.zeros((k, test.shape[0]))\n",
    "    weights = np.zeros(k)\n",
    "\n",
    "    norm_train = data.drop(['Viability Fraction '], axis=1)\n",
    "\n",
    "    nCSN_test = norm(test.drop(['Viability Fraction '], axis=1),\n",
    "    norm_train)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(k):\n",
    "\n",
    "        if k == 1:\n",
    "            val = CSN_shuf\n",
    "            train = CSN_shuf\n",
    "        else:\n",
    "            val = CSN_shuf[valn*i:valn*(i+1)]\n",
    "            train = CSN_shuf[valn*(i+1):].append(CSN_shuf[:valn*i])\n",
    "\n",
    "        train_f = train.drop(['Viability Fraction '], axis=1)\n",
    "        val_f = val.drop(['Viability Fraction '], axis=1)\n",
    "\n",
    "        ntrain_f = norm(train_f, norm_train)\n",
    "        ntrain_l = train['Viability Fraction ']\n",
    "        nval_f = norm(val_f, norm_train)\n",
    "        nval_l = val['Viability Fraction ']\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        unique_seed = s[i]\n",
    "        np.random.seed(unique_seed)\n",
    "        tf.random.set_seed(unique_seed)\n",
    "\n",
    "        model = build_model(ntrain_f)\n",
    "\n",
    "        history = model.fit(ntrain_f,\n",
    "        ntrain_l,\n",
    "        validation_data=(nval_f, nval_l),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0)\n",
    "\n",
    "        #plot_mae(history)\n",
    "\n",
    "        val_pred[i] = model.predict(nCSN_test).flatten()\n",
    "        weights[i] = 1/history.history['val_mae'][-1]\n",
    "\n",
    "    WMean = np.average(val_pred, axis=0, weights=weights)\n",
    "    Werr = np.sqrt(np.average((WMean-val_pred)**2, weights=weights, axis=0))/np.sqrt(k)\n",
    "    print(k, time.time() - start_time)\n",
    "\n",
    "    return WMean, Werr\n",
    "\n",
    "###################################\n",
    "\n",
    "out = np.zeros((300, 4, 2, 18))\n",
    "\n",
    "for i in range(300):\n",
    "\n",
    "    np.random.seed(i)\n",
    "    CSN_hold  = sklearn.utils.shuffle(CSN_prepared, random_state=np.random.randint(0, 100000))\n",
    "    out[i] = np.array([ikfold(i, CSN_hold, CSN_new, np.random.randint(0, 100000, i))\n",
    "                 for i in [1, 4, 7, 10]])\n",
    "\n",
    "    print((i+1), '% complete')\n",
    "    out.dump('206_T18_300.pkl')\n",
    "\n",
    "###################################\n",
    "\n",
    "out.dump('206_T18_300.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#206 machine T18 all ReLU ran 1000 times for all k folds (bagging code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "\n",
    "import time\n",
    "\n",
    "###################################\n",
    "\n",
    "def norm(x, train_dataset):\n",
    "    train_stats = train_dataset.describe().transpose()\n",
    "    return (x - train_stats['mean']) / train_stats['std'].replace(to_replace=0, value=1)\n",
    "\n",
    "def build_model(data):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(5, activation=tf.nn.relu, input_shape=(data.shape[1],)))\n",
    "    for k in range(3-1):\n",
    "        model.add(keras.layers.Dense(5, activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=keras.optimizers.Adam(),\n",
    "                metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "\n",
    "###################################\n",
    "\n",
    "CSN_path = './'\n",
    "\n",
    "def load_CSN_data():\n",
    "    csv_path = CSN_path + \"Master_List_LCPLCP.csv\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "CSN = load_CSN_data()\n",
    "\n",
    "CSN = CSN.drop(['Example ID', 'Source', 'Figure ID', 'Data Provider', 'PI',\n",
    "       'Date Received', 'Data Measurment Published', 'Prior Exposure', 'Comments', 'Error'], axis=1)\n",
    "\n",
    "tsize = CSN.shape[0]//10\n",
    "\n",
    "CSN_prepared = pd.get_dummies(CSN)\n",
    "\n",
    "CSN_prepared['Surface Area per Liter'] = CSN_prepared['Surface Area (NMC) (m2/g)'] * CSN_prepared['Concentration (mg/L)']\n",
    "CSN_prepared = CSN_prepared.drop(['Surface Area (NMC) (m2/g)'], axis=1)\n",
    "\n",
    "CSN_prepared['log Concentration'] = np.log10(CSN_prepared['Concentration (mg/L)'] + 1e-9)\n",
    "CSN_prepared = CSN_prepared.drop(['Concentration (mg/L)'], axis=1)\n",
    "\n",
    "CSN_prepared = CSN_prepared[:-18]\n",
    "\n",
    "###################################\n",
    "\n",
    "CSN_path = './'\n",
    "\n",
    "def load_CSN_data():\n",
    "    csv_path = CSN_path + \"Master_List_LCPLCP.csv\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "CSN = load_CSN_data()\n",
    "\n",
    "CSN_new_err = CSN['Error'][-18:]\n",
    "\n",
    "CSN = CSN.drop(['Example ID', 'Source', 'Figure ID', 'Data Provider', 'PI',\n",
    "       'Date Received', 'Data Measurment Published', 'Prior Exposure', 'Comments', 'Error'], axis=1)\n",
    "\n",
    "#tsize = CSN.shape[0]//10\n",
    "\n",
    "CSN_new = pd.get_dummies(CSN)\n",
    "\n",
    "CSN_new['Surface Area per Liter'] = CSN_new['Surface Area (NMC) (m2/g)'] * CSN_new['Concentration (mg/L)']\n",
    "CSN_new = CSN_new.drop(['Surface Area (NMC) (m2/g)'], axis=1)\n",
    "\n",
    "CSN_new['log Concentration'] = np.log10(CSN_new['Concentration (mg/L)'] + 1e-9)\n",
    "CSN_new = CSN_new.drop(['Concentration (mg/L)'], axis=1)\n",
    "\n",
    "CSN_new = CSN_new[-18:]\n",
    "\n",
    "###################################\n",
    "\n",
    "def ikfold(k, data, test, s):\n",
    "\n",
    "    CSN_shuf = data#sklearn.utils.shuffle(data, random_state=25)\n",
    "    valn = data.shape[0]//k\n",
    "    vscores = []\n",
    "    tscores = []\n",
    "    vloss = []\n",
    "    tloss = []\n",
    "    val_pred = np.zeros((k, test.shape[0]))\n",
    "    weights = np.zeros(k)\n",
    "\n",
    "    norm_train = data.drop(['Viability Fraction '], axis=1)\n",
    "\n",
    "    nCSN_test = norm(test.drop(['Viability Fraction '], axis=1),\n",
    "    norm_train)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(k):\n",
    "\n",
    "        if k == 1:\n",
    "            val = CSN_shuf\n",
    "            train = CSN_shuf\n",
    "        else:\n",
    "            val = CSN_shuf[valn*i:valn*(i+1)]\n",
    "            train = CSN_shuf[valn*(i+1):].append(CSN_shuf[:valn*i])\n",
    "\n",
    "        train_f = train.drop(['Viability Fraction '], axis=1)\n",
    "        val_f = val.drop(['Viability Fraction '], axis=1)\n",
    "\n",
    "        ntrain_f = norm(train_f, norm_train)\n",
    "        ntrain_l = train['Viability Fraction ']\n",
    "        nval_f = norm(val_f, norm_train)\n",
    "        nval_l = val['Viability Fraction ']\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        unique_seed = s[i]\n",
    "        np.random.seed(unique_seed)\n",
    "        tf.random.set_seed(unique_seed)\n",
    "\n",
    "        model = build_model(ntrain_f)\n",
    "\n",
    "        history = model.fit(ntrain_f,\n",
    "        ntrain_l,\n",
    "        validation_data=(nval_f, nval_l),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0)\n",
    "\n",
    "        #plot_mae(history)\n",
    "\n",
    "        val_pred[i] = model.predict(nCSN_test).flatten()\n",
    "        weights[i] = 1/history.history['val_mae'][-1]\n",
    "\n",
    "    WMean = np.average(val_pred, axis=0, weights=weights)\n",
    "    Werr = np.sqrt(np.average((WMean-val_pred)**2, weights=weights, axis=0))/np.sqrt(k)\n",
    "    print(k, time.time() - start_time)\n",
    "\n",
    "    return WMean, Werr\n",
    "\n",
    "###################################\n",
    "\n",
    "out = np.zeros((1000, 4, 2, 18))\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    np.random.seed(i)\n",
    "    CSN_hold  = sklearn.utils.shuffle(CSN_prepared, random_state=np.random.randint(0, 100000))\n",
    "    out[i] = np.array([ikfold(i, CSN_hold, CSN_new, np.random.randint(0, 100000, i))\n",
    "                 for i in [1, 4, 7, 10]])\n",
    "\n",
    "    print((i+1), '% complete')\n",
    "    out.dump('206_T18_1000.pkl')\n",
    "\n",
    "###################################\n",
    "\n",
    "out.dump('206_T18_1000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#186 machine T18 all ReLU ran 100 times for all k folds (bagging code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "\n",
    "import time\n",
    "########################\n",
    "def norm(x, train_dataset):\n",
    "    train_stats = train_dataset.describe().transpose()\n",
    "    return (x - train_stats['mean']) / train_stats['std'].replace(to_replace=0, value=1)\n",
    "\n",
    "def build_model(data):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(5, activation=tf.nn.relu, input_shape=(data.shape[1],)))\n",
    "    for k in range(3-1):\n",
    "        model.add(keras.layers.Dense(5, activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=keras.optimizers.Adam(),\n",
    "                metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "#########################\n",
    "CSN_path = './'\n",
    "#CSN_path = './Data/'\n",
    "\n",
    "def load_CSN_data():\n",
    "    csv_path = CSN_path + \"Master_List_LCPLCP.csv\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "CSN = load_CSN_data()\n",
    "\n",
    "CSN_new_err = CSN['Error'][-18:]\n",
    "\n",
    "CSN = CSN.drop(['Example ID', 'Source', 'Figure ID', 'Data Provider', 'PI',\n",
    "       'Date Received', 'Data Measurment Published', 'Prior Exposure', 'Comments', 'Error'], axis=1)\n",
    "\n",
    "tsize = CSN.shape[0]//10\n",
    "\n",
    "CSN_new = pd.get_dummies(CSN)\n",
    "\n",
    "CSN_new['Surface Area per Liter'] = CSN_new['Surface Area (NMC) (m2/g)'] * CSN_new['Concentration (mg/L)']\n",
    "CSN_new = CSN_new.drop(['Surface Area (NMC) (m2/g)'], axis=1)\n",
    "\n",
    "CSN_new['log Concentration'] = np.log10(CSN_new['Concentration (mg/L)'] + 1e-9)\n",
    "CSN_new = CSN_new.drop(['Concentration (mg/L)'], axis=1)\n",
    "\n",
    "CSN_new_A = CSN_new[-18:] #assign the last 18 examples for 18 test cases for a different array\n",
    "\n",
    "CSN_prepared_B = CSN_new[:-18] # Removing the last 18 \n",
    "CSN_hold_1 = sklearn.utils.shuffle(CSN_prepared_B, random_state=5946) #shuffling the 206 data examples as Clyde did\n",
    "tsize1 = CSN_hold_1.shape[0]//10 # decide the size of the test examples which is 20 - clyde did it\n",
    "\n",
    "CSN_prepared_B = CSN_hold_1[:-tsize1] #Remove that 20 data examples from the bottom of the list\n",
    "\n",
    "###########################\n",
    "\n",
    "def ikfold(k, data, test, s):\n",
    "\n",
    "    CSN_shuf = data#sklearn.utils.shuffle(data, random_state=25)\n",
    "    #CSN_shuf = CSN_shuf[20:]\n",
    "    valn = CSN_shuf.shape[0]//k\n",
    "    vscores = []\n",
    "    tscores = []\n",
    "    vloss = []\n",
    "    tloss = []\n",
    "    val_pred = np.zeros((k, test.shape[0]))\n",
    "    weights = np.zeros(k)\n",
    "\n",
    "    norm_train = CSN_shuf.drop(['Viability Fraction '], axis=1)\n",
    "\n",
    "    nCSN_test = norm(test.drop(['Viability Fraction '], axis=1),\n",
    "    norm_train)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(k):\n",
    "\n",
    "        if k == 1:\n",
    "            val = CSN_shuf\n",
    "            train = CSN_shuf\n",
    "        else:\n",
    "            val = CSN_shuf[valn*i:valn*(i+1)]\n",
    "            train = CSN_shuf[valn*(i+1):].append(CSN_shuf[:valn*i])\n",
    "\n",
    "        train_f = train.drop(['Viability Fraction '], axis=1)\n",
    "        val_f = val.drop(['Viability Fraction '], axis=1)\n",
    "\n",
    "        ntrain_f = norm(train_f, norm_train)\n",
    "        ntrain_l = train['Viability Fraction ']\n",
    "        nval_f = norm(val_f, norm_train)\n",
    "        nval_l = val['Viability Fraction ']\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        unique_seed = s[i]\n",
    "        np.random.seed(unique_seed)\n",
    "        tf.random.set_seed(unique_seed)\n",
    "\n",
    "        model = build_model(ntrain_f)\n",
    "\n",
    "        history = model.fit(ntrain_f,\n",
    "        ntrain_l,\n",
    "        validation_data=(nval_f, nval_l),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0)\n",
    "\n",
    "        #plot_mae(history)\n",
    "\n",
    "        val_pred[i] = model.predict(nCSN_test).flatten()\n",
    "        weights[i] = 1/history.history['val_mae'][-1]\n",
    "\n",
    "    WMean = np.average(val_pred, axis=0, weights=weights)\n",
    "    Werr = np.sqrt(np.average((WMean-val_pred)**2, weights=weights, axis=0))/np.sqrt(k)\n",
    "    print(k, time.time() - start_time)\n",
    "\n",
    "    return WMean, Werr\n",
    "#########################\n",
    "out = np.zeros((100, 4, 2, 18))\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    np.random.seed(i)\n",
    "    CSN_hold  = sklearn.utils.shuffle(CSN_prepared_B, random_state=np.random.randint(0, 100000))\n",
    "    out[i] = np.array([ikfold(i, CSN_hold, CSN_new_A, np.random.randint(0, 100000, i))\n",
    "                 for i in [1, 4, 7, 10]])\n",
    "\n",
    "    print((i+1), '% complete')\n",
    "    out.dump('186_T18.pkl')\n",
    "###########################\n",
    "\n",
    "out.dump('186_T18.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#186 machine T20 all ReLU ran 100 times for all k folds (bagging code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "\n",
    "import time\n",
    "########################\n",
    "def norm(x, train_dataset):\n",
    "    train_stats = train_dataset.describe().transpose()\n",
    "    return (x - train_stats['mean']) / train_stats['std'].replace(to_replace=0, value=1)\n",
    "\n",
    "def build_model(data):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(5, activation=tf.nn.relu, input_shape=(data.shape[1],)))\n",
    "    for k in range(3-1):\n",
    "        model.add(keras.layers.Dense(5, activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=keras.optimizers.Adam(),\n",
    "                metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "#########################\n",
    "CSN_path = './'\n",
    "#CSN_path = './Data/'\n",
    "\n",
    "def load_CSN_data():\n",
    "    csv_path = CSN_path + \"Master_List_LCPLCP.csv\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "CSN = load_CSN_data()\n",
    "\n",
    "CSN_new_err = CSN['Error'][-18:]\n",
    "\n",
    "CSN = CSN.drop(['Example ID', 'Source', 'Figure ID', 'Data Provider', 'PI',\n",
    "       'Date Received', 'Data Measurment Published', 'Prior Exposure', 'Comments', 'Error'], axis=1)\n",
    "\n",
    "tsize = CSN.shape[0]//10\n",
    "\n",
    "CSN_new = pd.get_dummies(CSN)\n",
    "\n",
    "CSN_new['Surface Area per Liter'] = CSN_new['Surface Area (NMC) (m2/g)'] * CSN_new['Concentration (mg/L)']\n",
    "CSN_new = CSN_new.drop(['Surface Area (NMC) (m2/g)'], axis=1)\n",
    "\n",
    "CSN_new['log Concentration'] = np.log10(CSN_new['Concentration (mg/L)'] + 1e-9)\n",
    "CSN_new = CSN_new.drop(['Concentration (mg/L)'], axis=1)\n",
    "\n",
    "CSN_new_A = CSN_new[-18:] #assign the last 18 examples for 18 test cases for a different array\n",
    "\n",
    "CSN_prepared_B = CSN_new[:-18] # Removing the last 18 \n",
    "CSN_hold_1 = sklearn.utils.shuffle(CSN_prepared_B, random_state=5946) #shuffling the 206 data examples as Clyde did\n",
    "tsize1 = CSN_hold_1.shape[0]//10 # decide the size of the test examples which is 20 - clyde did it\n",
    "\n",
    "CSN_prepared_B = CSN_hold_1[:-tsize1] #Remove that 20 data examples from the bottom of the list\n",
    "CSN_test = CSN_hold_1[-tsize1:] # assign that 20 data examples whcih we will use later\n",
    "###########################\n",
    "def ikfold(k, data, test, s):\n",
    "\n",
    "    CSN_shuf = data#sklearn.utils.shuffle(data, random_state=25)\n",
    "    #CSN_shuf = CSN_shuf[20:]\n",
    "    valn = CSN_shuf.shape[0]//k\n",
    "    vscores = []\n",
    "    tscores = []\n",
    "    vloss = []\n",
    "    tloss = []\n",
    "    val_pred = np.zeros((k, test.shape[0]))\n",
    "    weights = np.zeros(k)\n",
    "\n",
    "    norm_train = CSN_shuf.drop(['Viability Fraction '], axis=1)\n",
    "\n",
    "    nCSN_test = norm(test.drop(['Viability Fraction '], axis=1),\n",
    "    norm_train)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(k):\n",
    "\n",
    "        if k == 1:\n",
    "            val = CSN_shuf\n",
    "            train = CSN_shuf\n",
    "        else:\n",
    "            val = CSN_shuf[valn*i:valn*(i+1)]\n",
    "            train = CSN_shuf[valn*(i+1):].append(CSN_shuf[:valn*i])\n",
    "\n",
    "        train_f = train.drop(['Viability Fraction '], axis=1)\n",
    "        val_f = val.drop(['Viability Fraction '], axis=1)\n",
    "\n",
    "        ntrain_f = norm(train_f, norm_train)\n",
    "        ntrain_l = train['Viability Fraction ']\n",
    "        nval_f = norm(val_f, norm_train)\n",
    "        nval_l = val['Viability Fraction ']\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        unique_seed = s[i]\n",
    "        np.random.seed(unique_seed)\n",
    "        tf.random.set_seed(unique_seed)\n",
    "\n",
    "        model = build_model(ntrain_f)\n",
    "\n",
    "        history = model.fit(ntrain_f,\n",
    "        ntrain_l,\n",
    "        validation_data=(nval_f, nval_l),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0)\n",
    "\n",
    "        #plot_mae(history)\n",
    "\n",
    "        val_pred[i] = model.predict(nCSN_test).flatten()\n",
    "        weights[i] = 1/history.history['val_mae'][-1]\n",
    "\n",
    "    WMean = np.average(val_pred, axis=0, weights=weights)\n",
    "    Werr = np.sqrt(np.average((WMean-val_pred)**2, weights=weights, axis=0))/np.sqrt(k)\n",
    "    print(k, time.time() - start_time)\n",
    "\n",
    "    return WMean, Werr\n",
    "#########################\n",
    "out = np.zeros((100, 4, 2, 20))\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    np.random.seed(i)\n",
    "    CSN_hold  = sklearn.utils.shuffle(CSN_prepared_B, random_state=np.random.randint(0, 100000))\n",
    "    out[i] = np.array([ikfold(i, CSN_hold, CSN_test, np.random.randint(0, 100000, i))\n",
    "                 for i in [1, 4, 7, 10]])\n",
    "\n",
    "    print((i+1), '% complete')\n",
    "    out.dump('186_T20.pkl')\n",
    "###########################\n",
    "\n",
    "out.dump('186_T20.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#186 machine T20 all ReLU ran 100 times for k fold = 4 (bagging code) with 1000 epochs and batchsize 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "\n",
    "import time\n",
    "\n",
    "pd.set_option('max_columns', 100)\n",
    "###################################\n",
    "def norm(x, train_dataset):\n",
    "    train_stats = train_dataset.describe().transpose()\n",
    "    return (x - train_stats['mean']) / train_stats['std'].replace(to_replace=0, value=1)\n",
    "\n",
    "\n",
    "# this is the model used in Clyde's bagging code   \n",
    "        \n",
    "def build_model(data):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(5, activation=tf.nn.relu, input_shape=(data.shape[1],)))\n",
    "    for k in range(3-1):\n",
    "        model.add(keras.layers.Dense(5, activation=tf.nn.relu))\n",
    "    model.add(keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=keras.optimizers.Adam(),\n",
    "                metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 25\n",
    "\n",
    "\n",
    "###################################\n",
    "\n",
    "CSN_path = './'\n",
    "#CSN_path = './Data/'\n",
    "\n",
    "def load_CSN_data():\n",
    "    csv_path = CSN_path + \"Master_List_LCPLCP.csv\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "CSN = load_CSN_data()\n",
    "\n",
    "CSN_new_err = CSN['Error'][-18:]\n",
    "\n",
    "CSN = CSN.drop(['Example ID', 'Source', 'Figure ID', 'Data Provider', 'PI',\n",
    "       'Date Received', 'Data Measurment Published', 'Prior Exposure', 'Comments', 'Error'], axis=1)\n",
    "\n",
    "tsize = CSN.shape[0]//10\n",
    "\n",
    "CSN_new = pd.get_dummies(CSN)\n",
    "\n",
    "CSN_new['Surface Area per Liter'] = CSN_new['Surface Area (NMC) (m2/g)'] * CSN_new['Concentration (mg/L)']\n",
    "CSN_new = CSN_new.drop(['Surface Area (NMC) (m2/g)'], axis=1)\n",
    "\n",
    "CSN_new['log Concentration'] = np.log10(CSN_new['Concentration (mg/L)'] + 1e-9)\n",
    "CSN_new = CSN_new.drop(['Concentration (mg/L)'], axis=1)\n",
    "\n",
    "CSN_new_A = CSN_new[-18:] #assign the last 18 examples for 18 test cases for a different array\n",
    "\n",
    "CSN_prepared_B = CSN_new[:-18] # Removing the last 18 \n",
    "CSN_hold_1 = sklearn.utils.shuffle(CSN_prepared_B, random_state=5946) #shuffling the 206 data examples as Clyde did\n",
    "tsize1 = CSN_hold_1.shape[0]//10 # decide the size of the test examples which is 20 - clyde did it\n",
    "\n",
    "CSN_prepared_B = CSN_hold_1[:-tsize1] #Remove that 20 data examples from the bottom of the list\n",
    "CSN_test = CSN_hold_1[-tsize1:] # assign that 20 data examples whcih we will use later\n",
    "###################################\n",
    "def ikfold(k, data, test, s):\n",
    "\n",
    "    CSN_shuf = data#sklearn.utils.shuffle(data, random_state=25)\n",
    "    valn = data.shape[0]//k\n",
    "    vscores = []\n",
    "    tscores = []\n",
    "    vloss = []\n",
    "    tloss = []\n",
    "    val_pred = np.zeros((k, test.shape[0]))\n",
    "    weights = np.zeros(k)\n",
    "\n",
    "    norm_train = data.drop(['Viability Fraction '], axis=1)\n",
    "\n",
    "    nCSN_test = norm(test.drop(['Viability Fraction '], axis=1),\n",
    "    norm_train)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(k):\n",
    "\n",
    "        if k == 1:\n",
    "            val = CSN_shuf\n",
    "            train = CSN_shuf\n",
    "        else:\n",
    "            val = CSN_shuf[valn*i:valn*(i+1)]\n",
    "            train = CSN_shuf[valn*(i+1):].append(CSN_shuf[:valn*i])\n",
    "\n",
    "        train_f = train.drop(['Viability Fraction '], axis=1)\n",
    "        val_f = val.drop(['Viability Fraction '], axis=1)\n",
    "\n",
    "        ntrain_f = norm(train_f, norm_train)\n",
    "        ntrain_l = train['Viability Fraction ']\n",
    "        nval_f = norm(val_f, norm_train)\n",
    "        nval_l = val['Viability Fraction ']\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        unique_seed = s[i]\n",
    "        np.random.seed(unique_seed)\n",
    "        tf.random.set_seed(unique_seed)\n",
    "\n",
    "        model = build_model(ntrain_f)\n",
    "\n",
    "        history = model.fit(ntrain_f,\n",
    "        ntrain_l,\n",
    "        validation_data=(nval_f, nval_l),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0)\n",
    "\n",
    "        #plot_mae(history)\n",
    "\n",
    "        val_pred[i] = model.predict(nCSN_test).flatten()\n",
    "        weights[i] = 1/history.history['val_mae'][-1]\n",
    "\n",
    "    WMean = np.average(val_pred, axis=0, weights=weights)\n",
    "    Werr = np.sqrt(np.average((WMean-val_pred)**2, weights=weights, axis=0))/np.sqrt(k)\n",
    "    print(k, time.time() - start_time)\n",
    "    \n",
    "    return WMean, Werr\n",
    "###################################\n",
    "out = np.zeros((100, 1, 2, 20))\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    np.random.seed(i)\n",
    "    CSN_hold  = sklearn.utils.shuffle(CSN_prepared_B, random_state=np.random.randint(0, 100000))\n",
    "    out[i] = np.array([ikfold(i, CSN_hold, CSN_test, np.random.randint(0, 100000, i))\n",
    "                 for i in [4]])\n",
    "\n",
    "    print((i+1), '% complete')\n",
    "    out.dump('86_T20_1000epochs_25batchsize.pkl')\n",
    "###################################\n",
    "\n",
    "out.dump('186_T20_1000epochs_25batchsize.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above codes all use activation function ReLU. We later replaced the ReLU activation function with ELU/ReLU and the above\n",
    "# def build_model(data)function was replaced by the following function\n",
    "\n",
    "def build_model(data):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(5, activation=tf.nn.elu, input_shape=(data.shape[1],)))\n",
    "    for k in range(3-1):\n",
    "        model.add(keras.layers.Dense(5, activation=tf.nn.elu))\n",
    "    model.add(keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=keras.optimizers.Adam(),\n",
    "                metrics=['mae'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
